{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shaurya.rawat/Documents/mlplatform/feature-store/.venv/lib/python3.8/site-packages/feast/stream_feature_view.py:95: RuntimeWarning: Stream feature views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n",
      "/Users/shaurya.rawat/Documents/mlplatform/feature-store/.venv/lib/python3.8/site-packages/feast/infra/offline_stores/file_source.py:161: FutureWarning: 'ParquetDataset.schema' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.schema' attribute instead (which will return an Arrow schema instead of a Parquet schema).\n",
      "  schema = ParquetDataset(path).schema.to_arrow_schema()\n",
      "Updated feature view \u001b[1m\u001b[33mcustomer_stats\u001b[0m\n",
      "\tttl: \u001b[1m\u001b[33mseconds: 7776000\n",
      "\u001b[0m -> \u001b[1m\u001b[92mseconds: 8640000000\n",
      "\u001b[0m\n",
      "Updated feature view \u001b[1m\u001b[33mdriver_stats\u001b[0m\n",
      "\tttl: \u001b[1m\u001b[33mseconds: 7776000\n",
      "\u001b[0m -> \u001b[1m\u001b[92mseconds: 8640000000\n",
      "\u001b[0m\n",
      "Updated stream feature view \u001b[1m\u001b[33mdriver_hourly_stats_stream\u001b[0m\n",
      "\tttl: \u001b[1m\u001b[33mseconds: 7776000\n",
      "\u001b[0m -> \u001b[1m\u001b[92mseconds: 8640000000\n",
      "\u001b[0m\n",
      "\tuser_defined_function.body_text: \u001b[1m\u001b[33m@stream_feature_view(\n",
      "    entities=[driver],\n",
      "    ttl=timedelta(days=90),\n",
      "    mode=\"spark\",\n",
      "    schema=[\n",
      "        Field(name=\"conv_percentage\", dtype=Float32),\n",
      "        Field(name=\"acc_percentage\", dtype=Float32),\n",
      "    ],\n",
      "    timestamp_field=\"event_timestamp\",\n",
      "    online=True,\n",
      "    source=driver_stats_stream_source,\n",
      "    tags={\"stream\": \"True\"},\n",
      ")\n",
      "def driver_hourly_stats_stream(df: DataFrame):\n",
      "    # Define UDF here\n",
      "    from pyspark.sql.functions import col\n",
      "\n",
      "    return (\n",
      "        df.withColumn(\"conv_percentage\", col(\"conv_rate\") * 100.0)\n",
      "        .withColumn(\"acc_percentage\", col(\"acc_rate\") * 100.0)\n",
      "        .drop(\"conv_rate\", \"acc_rate\")\n",
      "    )\n",
      "\u001b[0m -> \u001b[1m\u001b[92m@stream_feature_view(\n",
      "    entities=[driver],\n",
      "    ttl=timedelta(days=100000),\n",
      "    mode=\"spark\",\n",
      "    schema=[\n",
      "        Field(name=\"conv_percentage\", dtype=Float32),\n",
      "        Field(name=\"acc_percentage\", dtype=Float32),\n",
      "    ],\n",
      "    timestamp_field=\"event_timestamp\",\n",
      "    online=True,\n",
      "    source=driver_stats_stream_source,\n",
      "    tags={\"stream\": \"True\"},\n",
      ")\n",
      "def driver_hourly_stats_stream(df: DataFrame):\n",
      "    # Define UDF here\n",
      "    from pyspark.sql.functions import col\n",
      "\n",
      "    return (\n",
      "        df.withColumn(\"conv_percentage\", col(\"conv_rate\") * 100.0)\n",
      "        .withColumn(\"acc_percentage\", col(\"acc_rate\") * 100.0)\n",
      "        .drop(\"conv_rate\", \"acc_rate\")\n",
      "    )\n",
      "\u001b[0m\n",
      "\n",
      "Deploying infrastructure for \u001b[1m\u001b[32mcustomer_stats\u001b[0m\n",
      "Deploying infrastructure for \u001b[1m\u001b[32mdriver_stats\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!feast apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "store = FeatureStore(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\\n  \"spec\": {\\n    \"name\": \"driver\",\\n    \"description\": \"Driver ID\",\\n    \"joinKey\": \"driver_id\"\\n  },\\n  \"meta\": {\\n    \"createdTimestamp\": \"2023-02-01T15:52:34.143640Z\",\\n    \"lastUpdatedTimestamp\": \"2023-02-01T15:52:34.143640Z\"\\n  }\\n}',\n",
       " '{\\n  \"spec\": {\\n    \"name\": \"customer\",\\n    \"description\": \"Customer ID\",\\n    \"joinKey\": \"customer_id\"\\n  },\\n  \"meta\": {\\n    \"createdTimestamp\": \"2023-02-01T15:52:34.143679Z\",\\n    \"lastUpdatedTimestamp\": \"2023-02-01T15:52:34.143679Z\"\\n  }\\n}',\n",
       " '{\\n  \"spec\": {\\n    \"name\": \"__dummy\",\\n    \"joinKey\": \"__dummy_id\"\\n  },\\n  \"meta\": {\\n    \"createdTimestamp\": \"2023-02-01T15:52:34.143590Z\",\\n    \"lastUpdatedTimestamp\": \"2023-02-01T15:52:34.143716Z\"\\n  }\\n}']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: str(x), store.registry.list_entities(project=\"hybrid_store\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<StreamFeatureView(aggregations = [], mode = spark, timestamp_field = event_timestamp, udf = <function driver_hourly_stats_stream at 0x7fed48e47ee0>, udf_string = @stream_feature_view(\n",
       "     entities=[driver],\n",
       "     ttl=timedelta(days=100000),\n",
       "     mode=\"spark\",\n",
       "     schema=[\n",
       "         Field(name=\"conv_percentage\", dtype=Float32),\n",
       "         Field(name=\"acc_percentage\", dtype=Float32),\n",
       "     ],\n",
       "     timestamp_field=\"event_timestamp\",\n",
       "     online=True,\n",
       "     source=driver_stats_stream_source,\n",
       "     tags={\"stream\": \"True\"},\n",
       " )\n",
       " def driver_hourly_stats_stream(df: DataFrame):\n",
       "     # Define UDF here\n",
       "     from pyspark.sql.functions import col\n",
       " \n",
       "     return (\n",
       "         df.withColumn(\"conv_percentage\", col(\"conv_rate\") * 100.0)\n",
       "         .withColumn(\"acc_percentage\", col(\"acc_rate\") * 100.0)\n",
       "         .drop(\"conv_rate\", \"acc_rate\")\n",
       "     )\n",
       " , name = driver_hourly_stats_stream, entities = ['driver'], ttl = 100000 days, 0:00:00, stream_source = {\n",
       "   \"type\": \"STREAM_KAFKA\",\n",
       "   \"timestampField\": \"event_timestamp\",\n",
       "   \"kafkaOptions\": {\n",
       "     \"kafkaBootstrapServers\": \"localhost:9092\",\n",
       "     \"topic\": \"drivers\",\n",
       "     \"messageFormat\": {\n",
       "       \"jsonFormat\": {\n",
       "         \"schemaJson\": \"driver_id integer, event_timestamp timestamp, conv_rate double, acc_rate double, created timestamp\"\n",
       "       }\n",
       "     },\n",
       "     \"watermarkDelayThreshold\": \"300s\"\n",
       "   },\n",
       "   \"name\": \"driver_stats_stream\",\n",
       "   \"description\": \"Kafka stream for driver stats\",\n",
       "   \"owner\": \"shaurya@xing.com\",\n",
       "   \"batchSource\": {\n",
       "     \"type\": \"BATCH_FILE\",\n",
       "     \"timestampField\": \"event_timestamp\",\n",
       "     \"createdTimestampColumn\": \"created\",\n",
       "     \"fileOptions\": {\n",
       "       \"uri\": \"data/driver_stats.parquet\"\n",
       "     },\n",
       "     \"name\": \"driver_stats_source\",\n",
       "     \"description\": \"Driver stats\",\n",
       "     \"owner\": \"shaurya@xing.com\"\n",
       "   }\n",
       " }, batch_source = {\n",
       "   \"type\": \"BATCH_FILE\",\n",
       "   \"timestampField\": \"event_timestamp\",\n",
       "   \"createdTimestampColumn\": \"created\",\n",
       "   \"fileOptions\": {\n",
       "     \"uri\": \"data/driver_stats.parquet\"\n",
       "   },\n",
       "   \"name\": \"driver_stats_source\",\n",
       "   \"description\": \"Driver stats\",\n",
       "   \"owner\": \"shaurya@xing.com\"\n",
       " }, entity_columns = [driver_id-Int64], features = [conv_percentage-Float32, acc_percentage-Float32, driver_id-Int64], description = , tags = {'stream': 'True'}, owner = , projection = FeatureViewProjection(name='driver_hourly_stats_stream', name_alias=None, desired_features=[], features=[conv_percentage-Float32, acc_percentage-Float32, driver_id-Int64], join_key_map={}), created_timestamp = 2023-02-01 15:52:34.141776, last_updated_timestamp = 2023-02-02 12:59:33.677694, online = True, materialization_intervals = [(datetime.datetime(1749, 4, 18, 16, 34, 26, 648895, tzinfo=<UTC>), datetime.datetime(2023, 2, 1, 17, 29, 26, 209780, tzinfo=<UTC>)), (datetime.datetime(2023, 2, 1, 17, 29, 26, 209780, tzinfo=<UTC>), datetime.datetime(2023, 2, 2, 10, 14, 1, 4466, tzinfo=<UTC>)), (datetime.datetime(2023, 2, 2, 10, 14, 1, 4466, tzinfo=<UTC>), datetime.datetime(2023, 2, 2, 13, 54, 33, 490818, tzinfo=<UTC>))])>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.registry.list_stream_feature_views(project=\"hybrid_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customer_id                  event_timestamp  current_balance  \\\n",
      "0         5001        2021-04-12 10:59:42+00:00         0.174109   \n",
      "1         5003        2021-04-12 16:40:26+00:00         0.735872   \n",
      "2         5004        2021-04-12 15:01:12+00:00         0.885541   \n",
      "3         5002        2021-04-12 08:12:10+00:00         0.922777   \n",
      "4         5005 2023-02-02 14:01:50.296475+00:00         0.928581   \n",
      "\n",
      "   avg_passenger_count  lifetime_trip_count  \n",
      "0             0.384933                   14  \n",
      "1             0.542926                  616  \n",
      "2             0.774241                  129  \n",
      "3             0.167704                  844  \n",
      "4             0.883550                  399  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "entity_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"customer_id\": [5001, 5002, 5003, 5004, 5005],\n",
    "        \"event_timestamp\": [\n",
    "            datetime(2021, 4, 12, 10, 59, 42),\n",
    "            datetime(2021, 4, 12, 8, 12, 10),\n",
    "            datetime(2021, 4, 12, 16, 40, 26),\n",
    "            datetime(2021, 4, 12, 15, 1, 12),\n",
    "            datetime.now()\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"customer_stats:current_balance\",\n",
    "        \"customer_stats:avg_passenger_count\",\n",
    "        \"customer_stats:lifetime_trip_count\"\n",
    "    ]\n",
    ").to_df()\n",
    "\n",
    "print(training_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>driver_id</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>conv_percentage</th>\n",
       "      <th>acc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>2021-04-12 10:59:42+00:00</td>\n",
       "      <td>52.114902</td>\n",
       "      <td>75.165855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>2021-04-12 16:40:26+00:00</td>\n",
       "      <td>18.885477</td>\n",
       "      <td>34.473606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1004</td>\n",
       "      <td>2021-04-12 15:01:12+00:00</td>\n",
       "      <td>29.649216</td>\n",
       "      <td>93.530525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002</td>\n",
       "      <td>2021-04-12 08:12:10+00:00</td>\n",
       "      <td>8.901370</td>\n",
       "      <td>21.263689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>2023-02-02 14:01:50.407645+00:00</td>\n",
       "      <td>40.458847</td>\n",
       "      <td>40.757076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   driver_id                  event_timestamp  conv_percentage  acc_percentage\n",
       "0       1001        2021-04-12 10:59:42+00:00        52.114902       75.165855\n",
       "1       1003        2021-04-12 16:40:26+00:00        18.885477       34.473606\n",
       "2       1004        2021-04-12 15:01:12+00:00        29.649216       93.530525\n",
       "3       1002        2021-04-12 08:12:10+00:00         8.901370       21.263689\n",
       "4       1001 2023-02-02 14:01:50.407645+00:00        40.458847       40.757076"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"driver_id\": [1001, 1002, 1003, 1004, 1001],\n",
    "        \"event_timestamp\": [\n",
    "            datetime(2021, 4, 12, 10, 59, 42),\n",
    "            datetime(2021, 4, 12, 8, 12, 10),\n",
    "            datetime(2021, 4, 12, 16, 40, 26),\n",
    "            datetime(2021, 4, 12, 15, 1, 12),\n",
    "            datetime.now()\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"driver_hourly_stats_stream:conv_percentage\",\n",
    "        \"driver_hourly_stats_stream:acc_percentage\"\n",
    "    ]\n",
    ").to_df()\n",
    "\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m3\u001b[0m feature views to \u001b[1m\u001b[32m2023-02-02 13:56:50+01:00\u001b[0m into the \u001b[1m\u001b[32mredis\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mcustomer_stats\u001b[0m from \u001b[1m\u001b[32m2023-02-02 14:54:33+01:00\u001b[0m to \u001b[1m\u001b[32m2023-02-02 13:56:50+01:00\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mdriver_stats\u001b[0m from \u001b[1m\u001b[32m2023-02-02 14:54:33+01:00\u001b[0m to \u001b[1m\u001b[32m2023-02-02 14:56:50+01:00\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mdriver_hourly_stats_stream\u001b[0m from \u001b[1m\u001b[32m2023-02-02 14:54:33+01:00\u001b[0m to \u001b[1m\u001b[32m2023-02-02 14:56:50+01:00\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "store.materialize_incremental(end_date=datetime.now() - timedelta(minutes=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'driver_id': [1001, 1002],\n",
       " 'driver_id__ts': [0, 0],\n",
       " 'conv_percentage': [40.45884704589844, 46.587459564208984],\n",
       " 'conv_percentage__ts': [1647266400, 1647266400],\n",
       " 'acc_percentage': [40.757076263427734, 31.57208824157715],\n",
       " 'acc_percentage__ts': [1647266400, 1647266400]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Online stores\n",
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats_stream:conv_percentage\",\n",
    "        \"driver_hourly_stats_stream:acc_percentage\"\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\"driver_id\": 1001},\n",
    "        {\"driver_id\": 1002}\n",
    "    ]\n",
    ").to_dict(include_event_timestamps=True)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/02 14:01:54 WARN Utils: Your hostname, nb-srawat.local resolves to a loopback address: 127.0.0.1; using 192.168.1.133 instead (on interface en0)\n",
      "23/02/02 14:01:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/shaurya.rawat/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/shaurya.rawat/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2e2da9e3-df46-4c99-b3ea-65ec5a026806;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in local-m2-cache\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in local-m2-cache\n",
      "\tfound org.lz4#lz4-java;1.7.1 in local-m2-cache\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in spark-list\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 4701ms :: artifacts dl 65ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from local-m2-cache in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from local-m2-cache in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from spark-list in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from local-m2-cache in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   3   |   3   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2e2da9e3-df46-4c99-b3ea-65ec5a026806\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/27ms)\n",
      "23/02/02 14:02:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Make sure stream  delivers fresh feature values to the online store for the entites it has been registered with\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell\"\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"feast-spark\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/02 14:02:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF cols: Index(['driver_id', 'event_timestamp', 'created', 'conv_percentage',\n",
      "       'acc_percentage'],\n",
      "      dtype='object')\n",
      "DF Size: 0\n",
      "DF head: Empty DataFrame\n",
      "Columns: [driver_id, event_timestamp, created, conv_percentage, acc_percentage]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF cols: Index(['driver_id', 'event_timestamp', 'created', 'conv_percentage',\n",
      "       'acc_percentage'],\n",
      "      dtype='object')\n",
      "DF Size: 25\n",
      "DF head:    driver_id     event_timestamp                          created  \\\n",
      "0       1001 2031-02-22 22:00:00 2023-02-02 13:02:39.297797+00:00   \n",
      "1       1002 2031-02-22 21:00:00 2023-02-02 13:02:39.297797+00:00   \n",
      "2       1003 2031-02-22 21:00:00 2023-02-02 13:02:39.297797+00:00   \n",
      "3       1004 2031-02-22 22:00:00 2023-02-02 13:02:39.297797+00:00   \n",
      "4       1005 2031-02-22 22:00:00 2023-02-02 13:02:39.297797+00:00   \n",
      "\n",
      "   conv_percentage  acc_percentage  \n",
      "0         6.938688        9.072059  \n",
      "1        18.959631       54.245543  \n",
      "2        96.221501       68.706465  \n",
      "3         0.420751       21.136233  \n",
      "4        57.497627       83.557701  \n",
      "DF cols: Index(['driver_id', 'event_timestamp', 'created', 'conv_percentage',\n",
      "       'acc_percentage'],\n",
      "      dtype='object')\n",
      "DF Size: 25\n",
      "DF head:    driver_id     event_timestamp                          created  \\\n",
      "0       1001 2024-02-25 15:00:00 2023-02-02 13:03:01.018239+00:00   \n",
      "1       1002 2024-02-25 15:00:00 2023-02-02 13:03:01.018239+00:00   \n",
      "2       1003 2024-02-25 15:00:00 2023-02-02 13:03:01.018239+00:00   \n",
      "3       1004 2024-02-25 15:00:00 2023-02-02 13:03:01.018239+00:00   \n",
      "4       1005 2024-02-25 15:00:00 2023-02-02 13:03:01.018239+00:00   \n",
      "\n",
      "   conv_percentage  acc_percentage  \n",
      "0        55.265898       70.706928  \n",
      "1        34.858552       95.438379  \n",
      "2        99.837506       51.883465  \n",
      "3        93.291509        2.339130  \n",
      "4        14.221419       40.898654  \n"
     ]
    }
   ],
   "source": [
    "from feast.data_source import PushMode\n",
    "from processor import ProcessorConfig, SparkProcessor\n",
    "\n",
    "def preprocess_fn(rows: pd.DataFrame):\n",
    "    print(f\"DF cols: {rows.columns}\")\n",
    "    print(f\"DF Size: {rows.size}\")\n",
    "    print(f\"DF head: {rows.head(5)}\")\n",
    "    return rows\n",
    "\n",
    "config = ProcessorConfig(mode=\"spark\", spark_session=spark, processing_time=\"30 seconds\", query_timeout=15)\n",
    "stream_fv = store.get_stream_feature_view(\"driver_hourly_stats_stream\")\n",
    "\n",
    "processor = SparkProcessor(\n",
    "    fs=store,\n",
    "    sfv=stream_fv,\n",
    "    config=config,\n",
    "    preprocess_fn=preprocess_fn\n",
    ")\n",
    "\n",
    "query = processor.ingest_stream_into_online_store(to=PushMode.ONLINE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'driver_id': [1001, 1003],\n",
       " 'driver_id__ts': [0, 0],\n",
       " 'conv_percentage': [6.938688278198242, 96.22150421142578],\n",
       " 'conv_percentage__ts': [1929564000, 1929560400],\n",
       " 'acc_percentage': [9.07205867767334, 68.70646667480469],\n",
       " 'acc_percentage__ts': [1929564000, 1929560400]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Online features after stream ingestion\n",
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats_stream:conv_percentage\",\n",
    "        \"driver_hourly_stats_stream:acc_percentage\"\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\"driver_id\": 1001},\n",
    "        {\"driver_id\": 1003}\n",
    "    ]\n",
    ").to_dict(include_event_timestamps=True)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/02 14:44:35 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 2023276 ms exceeds timeout 120000 ms\n",
      "23/02/02 14:44:35 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>driver_id</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>conv_percentage</th>\n",
       "      <th>acc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>2027-03-20 10:59:42+00:00</td>\n",
       "      <td>40.458847</td>\n",
       "      <td>40.757076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1004</td>\n",
       "      <td>2027-03-20 10:59:42+00:00</td>\n",
       "      <td>97.727554</td>\n",
       "      <td>5.158236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>2027-03-20 10:59:42+00:00</td>\n",
       "      <td>86.991714</td>\n",
       "      <td>77.956184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002</td>\n",
       "      <td>2027-03-20 10:59:42+00:00</td>\n",
       "      <td>46.587460</td>\n",
       "      <td>31.572088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>2027-03-20 10:59:42+00:00</td>\n",
       "      <td>39.407246</td>\n",
       "      <td>4.611785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   driver_id           event_timestamp  conv_percentage  acc_percentage\n",
       "0       1001 2027-03-20 10:59:42+00:00        40.458847       40.757076\n",
       "1       1004 2027-03-20 10:59:42+00:00        97.727554        5.158236\n",
       "2       1003 2027-03-20 10:59:42+00:00        86.991714       77.956184\n",
       "3       1002 2027-03-20 10:59:42+00:00        46.587460       31.572088\n",
       "4       1005 2027-03-20 10:59:42+00:00        39.407246        4.611785"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_df = pd.DataFrame.from_dict({\n",
    "    \"driver_id\": [1001, 1002, 1003, 1004, 1005],\n",
    "        \"event_timestamp\": [\n",
    "            datetime(2027, 3, 20, 10, 59, 42),\n",
    "            datetime(2027, 3, 20, 10, 59, 42),\n",
    "            datetime(2027, 3, 20, 10, 59, 42),\n",
    "            datetime(2027, 3, 20, 10, 59, 42),\n",
    "            datetime(2027, 3, 20, 10, 59, 42),\n",
    "        ]\n",
    "})\n",
    "\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"driver_hourly_stats_stream:conv_percentage\",\n",
    "        \"driver_hourly_stats_stream:acc_percentage\"\n",
    "    ]\n",
    ").to_df()\n",
    "\n",
    "training_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/02 15:41:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF cols: Index(['driver_id', 'event_timestamp', 'created', 'conv_percentage',\n",
      "       'acc_percentage'],\n",
      "      dtype='object')\n",
      "DF Size: 25\n",
      "DF head:    driver_id     event_timestamp                          created  \\\n",
      "0       1001 2024-03-05 22:00:00 2023-02-02 14:41:45.957448+00:00   \n",
      "1       1002 2024-03-05 23:00:00 2023-02-02 14:41:45.957448+00:00   \n",
      "2       1003 2024-03-05 22:00:00 2023-02-02 14:41:45.957448+00:00   \n",
      "3       1004 2024-03-05 22:00:00 2023-02-02 14:41:45.957448+00:00   \n",
      "4       1005 2024-03-05 22:00:00 2023-02-02 14:41:45.957448+00:00   \n",
      "\n",
      "   conv_percentage  acc_percentage  \n",
      "0        16.334991       41.511256  \n",
      "1        62.255239       70.562595  \n",
      "2        51.607168       73.766935  \n",
      "3        69.630742       60.157454  \n",
      "4        68.359011       61.037648  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF cols: Index(['driver_id', 'event_timestamp', 'created', 'conv_percentage',\n",
      "       'acc_percentage'],\n",
      "      dtype='object')\n",
      "DF Size: 25\n",
      "DF head:    driver_id     event_timestamp                          created  \\\n",
      "0       1001 2024-03-06 01:00:00 2023-02-02 14:42:00.599749+00:00   \n",
      "1       1002 2024-03-06 01:00:00 2023-02-02 14:42:00.599749+00:00   \n",
      "2       1003 2024-03-06 02:00:00 2023-02-02 14:42:00.599749+00:00   \n",
      "3       1004 2024-03-06 01:00:00 2023-02-02 14:42:00.599749+00:00   \n",
      "4       1005 2024-03-06 01:00:00 2023-02-02 14:42:00.599749+00:00   \n",
      "\n",
      "   conv_percentage  acc_percentage  \n",
      "0        29.077673       10.660022  \n",
      "1        11.273976       44.519717  \n",
      "2        66.780871       14.266326  \n",
      "3        57.489824       95.271534  \n",
      "4        29.847521       11.959860  \n"
     ]
    }
   ],
   "source": [
    "query = processor.ingest_stream_into_online_store(PushMode.OFFLINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>driver_id</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>conv_percentage</th>\n",
       "      <th>acc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1004</td>\n",
       "      <td>2027-03-20 10:59:42+00:00</td>\n",
       "      <td>57.489822</td>\n",
       "      <td>95.271530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1005</td>\n",
       "      <td>2027-03-20 10:59:42+00:00</td>\n",
       "      <td>29.847521</td>\n",
       "      <td>11.959860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>2027-03-20 10:59:42+00:00</td>\n",
       "      <td>11.273976</td>\n",
       "      <td>44.519718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>2027-03-20 10:59:42+00:00</td>\n",
       "      <td>29.077673</td>\n",
       "      <td>10.660023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1003</td>\n",
       "      <td>2027-03-20 10:59:42+00:00</td>\n",
       "      <td>66.780869</td>\n",
       "      <td>14.266326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   driver_id           event_timestamp  conv_percentage  acc_percentage\n",
       "0       1004 2027-03-20 10:59:42+00:00        57.489822       95.271530\n",
       "1       1005 2027-03-20 10:59:42+00:00        29.847521       11.959860\n",
       "2       1002 2027-03-20 10:59:42+00:00        11.273976       44.519718\n",
       "3       1001 2027-03-20 10:59:42+00:00        29.077673       10.660023\n",
       "4       1003 2027-03-20 10:59:42+00:00        66.780869       14.266326"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF cols: Index(['driver_id', 'event_timestamp', 'created', 'conv_percentage',\n",
      "       'acc_percentage'],\n",
      "      dtype='object')\n",
      "DF Size: 25\n",
      "DF head:    driver_id     event_timestamp                          created  \\\n",
      "0       1001 2024-03-06 07:00:00 2023-02-02 14:42:30.541654+00:00   \n",
      "1       1002 2024-03-06 07:00:00 2023-02-02 14:42:30.541654+00:00   \n",
      "2       1003 2024-03-06 07:00:00 2023-02-02 14:42:30.541654+00:00   \n",
      "3       1004 2024-03-06 07:00:00 2023-02-02 14:42:30.541654+00:00   \n",
      "4       1005 2024-03-06 08:00:00 2023-02-02 14:42:30.541654+00:00   \n",
      "\n",
      "   conv_percentage  acc_percentage  \n",
      "0        10.988791       69.367623  \n",
      "1        77.247840       87.864637  \n",
      "2        78.075516        6.504741  \n",
      "3        44.163507       61.944288  \n",
      "4        53.096521       52.055502  \n"
     ]
    }
   ],
   "source": [
    "# Query after ingesting into the offline store\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"driver_hourly_stats_stream:conv_percentage\",\n",
    "        \"driver_hourly_stats_stream:acc_percentage\"\n",
    "    ]\n",
    ").to_df()\n",
    "\n",
    "training_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(\"/tmp/checkpoint\")\n",
    "except OSError as e:\n",
    "    print(\"Error: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0e517e424a48f31ab91b8c130a0638b80fd8696e0f9c4968490eb42cea8fcf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
